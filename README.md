# Sync Colombia Trademarks (Pipeline de ETL)

Este proyecto es un pipeline de ETL robusto diseÃ±ado para extraer, procesar y sincronizar datos de marcas registradas desde el portal SIPI (Superintendencia de Industria y Comercio) de Colombia.

Utiliza scraping web para obtener los datos, los normaliza y los carga en una base de datos PostgreSQL, asegurando que la informaciÃ³n local estÃ© actualizada.

## ğŸš€ CaracterÃ­sticas Principales

  * **Scraping Paralelo:** Ejecuta tareas de scraping en paralelo usando `multiprocessing` para maximizar la eficiencia (un proceso para Clases Niza e histÃ³ricos, otro para datos recientes).
  * **ETL por Lotes:** Procesa los archivos JSON generados en lotes (uno por uno) para realizar la carga en la base de datos, evitando sobrecargas de memoria.
  * **Auto-CorrecciÃ³n:** Incluye un flujo de verificaciÃ³n que identifica registros activos en la BD que faltan en los JSON (por fallos de scraping), los re-extrae individualmente por su nÃºmero de solicitud y actualiza sus estados.
  * **Reportes en S3:** Genera un reporte de cambios (`change_report.csv`) en cada ejecuciÃ³n y un reporte de registros faltantes (`missing_records.csv`) durante la correcciÃ³n, y los sube automÃ¡ticamente a un bucket de S3.
  * **Monitoreo de Errores:** Integrado con Rollbar para el monitoreo de excepciones y mensajes de estado en tiempo real.
  * **SimulaciÃ³n Humana:** Utiliza un `user_agent` y `viewport` especÃ­ficos para simular un navegador real (Chrome en Windows) y evitar bloqueos por parte del sitio de SIPI.

## âš™ï¸ Flujo del Proceso

El pipeline se ejecuta en el siguiente orden:

1.  **Inicio:** El proceso se invoca desde `src/handler/sync_colombia_trademarks.py`, que recibe el argumento `--status` (`active` o `inactive`).
2.  **Scraping Paralelo (`sync_orchestrator.py`):** Se lanzan dos procesos:
      * **Worker 1:** Extrae datos por Clases Niza (1-45) y datos histÃ³ricos (1900-2018).
      * **Worker 2:** Extrae datos recientes (2019-Presente) con una granularidad mÃ¡s fina (semanal y diaria).
      * Todos los resultados se guardan como archivos JSON en la carpeta temporal `tmp/`.
3.  **ETL Principal (`etl_functions.py`):**
      * Una vez que *ambos* workers de scraping terminan, el proceso principal lee los archivos JSON de `tmp/` en lotes.
      * Normaliza los datos (formatea fechas, estados, titulares).
      * Compara cada lote con la base de datos PostgreSQL, identificando registros nuevos y modificados.
      * Realiza `INSERT` para registros nuevos y `UPDATE` para registros existentes que cambiaron.
      * Genera y sube el `change_report.csv` a S3.
4.  **VerificaciÃ³n y CorrecciÃ³n (`etl_functions.py`):**
      * Compara todos los `request_number` de la BD (con estado activo) contra todos los `request_number` encontrados en los JSON.
      * Si encuentra registros en la BD que no estÃ¡n en los JSON, genera `missing_records.csv` y lo sube a S3.
      * Inicia un scraping secundario (`run_scraping_for_missing_requests`) que visita el sitio de SIPI y busca cada registro faltante por su nÃºmero.
      * Guarda los resultados de la correcciÃ³n en un nuevo JSON y actualiza los estados en la BD.
5.  **Limpieza:** Al finalizar todo el proceso, la carpeta `tmp/` y su contenido son eliminados.

## ğŸ› ï¸ TecnologÃ­as Utilizadas

  * Python
  * Playwright (para scraping web asÃ­ncrono)
  * Pandas (para manipulaciÃ³n de datos y generaciÃ³n de CSV)
  * Psycopg2 (para conectividad con PostgreSQL)
  * Boto3 (para conectividad con AWS S3)
  * Rollbar (para monitoreo de errores)
  * Multiprocessing (para paralelismo)

## ğŸ”§ ConfiguraciÃ³n

### 1\. Prerrequisitos

  * Python 3.8+
  * Una base de datos PostgreSQL accesible.
  * Credenciales de AWS (con permisos de escritura en S3).
  * Un token de acceso de Rollbar.

### 2\. InstalaciÃ³n

1.  Clona este repositorio:
    ```bash
    git clone [URL_DEL_REPOSITORIO]
    cd [NOMBRE_DEL_REPOSITORIO]
    ```
2.  (Recomendado) Crea y activa un entorno virtual:
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```
3.  Instala las dependencias:
    ```bash
    pip install -r requirements.txt
    ```
4.  Instala los navegadores necesarios para Playwright:
    ```bash
    playwright install
    ```

### 3\. Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto. Este archivo es **crÃ­tico** para la configuraciÃ³n de la aplicaciÃ³n.

```ini
# --- Base de Datos (PostgreSQL) ---
PG_USER="tu_usuario_db"
PG_PASS="tu_contraseÃ±a_db"
PG_HOST="tu_host_db"
PG_PORT="5432"
PG_DB="tu_nombre_db"
TABLE="nombre_de_la_tabla_marcas"

# --- Monitoreo (Rollbar) ---
ROLLBAR_TOKEN="tu_token_de_rollbar"
ENV_STAGE="development" # o "production"

# --- AWS (S3) ---
AWS_ACCESS_KEY_ID="tu_access_key_id"
AWS_SECRET_ACCESS_KEY="tu_secret_access_key"
AWS_REGION="tu_region_aws" # ej: "us-east-1"
```

## ğŸƒ CÃ³mo Ejecutar

El script principal es `src/handler/sync_colombia_trademarks.py`. Debe ejecutarse pasando el argumento `--status` para definir quÃ© tipo de marcas se van a scrapear.

**Para sincronizar marcas activas:**

```bash
python src/handler/sync_colombia_trademarks.py --status active
```

**Para sincronizar marcas inactivas:**

```bash
python src/handler/sync_colombia_trademarks.py --status inactive
```

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ functions/        # LÃ³gica de negocio principal
â”‚   â”‚   â”œâ”€â”€ etl_functions.py
â”‚   â”‚   â”œâ”€â”€ scraping_functions.py
â”‚   â”‚   â””â”€â”€ sync_orchestrator.py
â”‚   â”œâ”€â”€ gateways/         # MÃ³dulos para interactuar con servicios externos
â”‚   â”‚   â”œâ”€â”€ database_gateway.py
â”‚   â”‚   â”œâ”€â”€ s3_gateway.py
â”‚   â”‚   â””â”€â”€ scraping_gateway.py
â”‚   â”œâ”€â”€ handler/          # Punto de entrada de la aplicaciÃ³n
â”‚   â”‚   â””â”€â”€ sync_colombia_trademarks.py
â”‚   â”œâ”€â”€ middlewares/      # Configuraciones de middlewares
â”‚   â”‚   â””â”€â”€ rollbar_config.py
â”‚   â”œâ”€â”€ services/         # LÃ³gica del handler
â”‚   â”‚   â””â”€â”€ sync_colombia_trademarks/
â”‚   â”‚       â””â”€â”€ main.py
â”‚   â””â”€â”€ utils/            # Utilidades, constantes y normalizaciÃ³n
â”‚       â”œâ”€â”€ constants.py
â”‚       â”œâ”€â”€ data_normalizer.py
â”‚       â””â”€â”€ logging_config.py
â”œâ”€â”€ .env                  # (TÃº debes crearlo)
â”œâ”€â”€ etl_process_en.log    # (Generado en ejecuciÃ³n)
â””â”€â”€ requirements.txt      # (AsegÃºrate de tenerlo)
```